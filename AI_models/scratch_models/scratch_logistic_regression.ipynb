{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(self, degree=2, max_iter=10000, tol=0.001):\n",
    "        self.degree = degree\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.classes = None\n",
    "        self.cls_one_hot_encoding = None  # ground truth\n",
    "\n",
    "    def predict(self):\n",
    "        # Sigmoid function\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Gradient Descent\n",
    "        # transform data in degree 2 space: 1, a, b, a**2, a*b, b**2\n",
    "        if self.degree == 2: \n",
    "            # compute new feature space for nonlinear separator\n",
    "            poly = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "        else:\n",
    "            poly = X\n",
    "\n",
    "        self.classes = np.sort(np.unique(y))  # ascending order\n",
    "        self._init_weights(self.classes.shape[0], X.shape[1])  # Shape of (number of classes, number of features). Every class has his own weights vector\n",
    "        self._create_label_matrix(y)\n",
    "\n",
    "        y_head = self._sigmoid(self.weights, X)  # (4,150)\n",
    "        prob = self._softmax(y_head.T)  # Row vector corresponds to the probability distribution for one sample \n",
    "\n",
    "        self.weights = self.weights - self.tol * np.multiply((y_head - self.cls_one_hot_encoding) * X) ### ??? Shapes... weights = (3,4) - (matrix with 3,4)\n",
    "        # for gd: broadcast the multiply object to one higher dimension\n",
    "        loss = self._log_loss(prob)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _sigmoid(self, w, X):\n",
    "        Z = w @ X.T\n",
    "        y_head = 1 / (1 + np.exp(-Z))\n",
    "        return y_head\n",
    "    \n",
    "    # https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "    def _softmax(self, y_head):\n",
    "        assert len(y_head.shape) == 2\n",
    "        s = np.max(y_head, axis=1)\n",
    "        s = s[:, np.newaxis] # necessary step to do broadcasting. Transform array into next dimension as column vector\n",
    "        e_x = np.exp(y_head - s)\n",
    "        div = np.sum(e_x, axis=1)\n",
    "        div = div[:, np.newaxis]\n",
    "        return e_x / div\n",
    "\n",
    "    def _gd(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _log_loss(self, prob):\n",
    "        # label matrix \n",
    "        L = np.multiply(self.cls_one_hot_encoding, np.log(prob))\n",
    "        log_loss = -(np.sum(L)) / len(self.cls_one_hot_encoding)\n",
    "        return log_loss\n",
    "\n",
    "    def _create_label_matrix(self, y):\n",
    "        print(\"Classes: \", self.classes)\n",
    "        self.cls_one_hot_encoding = np.zeros((len(y), len(self.classes)))\n",
    "        for i, label in enumerate(y):\n",
    "            self.cls_one_hot_encoding[i][label] = 1\n",
    "\n",
    "    def _init_weights(self, num_cls, num_feat):\n",
    "        self.weights = abs(np.random.randn(num_cls, num_feat))\n",
    "\n",
    "clf = LogisticRegressionClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  [0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1285116393877443"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo = clf.fit(X,y)\n",
    "lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02147877, 0.18513703, 0.46064919, 1.8594102 ],\n",
       "       [1.31785408, 0.47056711, 0.15139916, 1.41771074],\n",
       "       [0.70883846, 1.16184783, 0.12142978, 1.26182261]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 30)\n",
      "(6, 30)\n",
      "[[0.95125229 0.9504006  0.7845668  0.95517707 0.92156035 0.93280626\n",
      "  0.78343239 0.96081771 0.96597665 0.9046949  0.93571762 0.93397689\n",
      "  0.95119931 0.93801024 0.94121477 0.9259426  0.89389733 0.95288163\n",
      "  0.91588766 0.96121476 0.88540659 0.9530323  0.91530862 0.95315505\n",
      "  0.93785739 0.90473745 0.9216464  0.93598583 0.9239716  0.9414486 ]\n",
      " [0.92746087 0.89752602 0.82339353 0.92977196 0.86733657 0.90075324\n",
      "  0.8258628  0.93285163 0.87542247 0.87169455 0.92934843 0.87875784\n",
      "  0.9407809  0.86886707 0.88821039 0.92921688 0.89380083 0.91905964\n",
      "  0.91464154 0.94505058 0.89879295 0.90794165 0.88178815 0.88395833\n",
      "  0.92737879 0.90361442 0.8939087  0.84462647 0.90317036 0.90875695]\n",
      " [0.96798413 0.9517135  0.86619871 0.95992541 0.94604325 0.95360704\n",
      "  0.88331074 0.97397428 0.96460837 0.93685126 0.964773   0.94165688\n",
      "  0.96934114 0.95441869 0.95565049 0.95858887 0.92623808 0.96680763\n",
      "  0.93919952 0.9807512  0.90939771 0.96540749 0.93375092 0.95292303\n",
      "  0.95561574 0.91878258 0.95297226 0.93864136 0.9238065  0.96110297]\n",
      " [0.93384878 0.93426068 0.79930743 0.93892276 0.89324128 0.91407433\n",
      "  0.74523658 0.92371711 0.94213409 0.91111135 0.90308817 0.8867824\n",
      "  0.91563118 0.91961764 0.90206455 0.88323479 0.86948179 0.93393831\n",
      "  0.90147879 0.93633763 0.9100023  0.90727897 0.91917276 0.91974767\n",
      "  0.89707946 0.90642852 0.91831607 0.88646134 0.88991862 0.92578116]\n",
      " [0.92823889 0.92523739 0.84321871 0.94744048 0.87876436 0.93360956\n",
      "  0.81100496 0.96239441 0.93575387 0.89471394 0.92058157 0.92668446\n",
      "  0.94293852 0.8975701  0.93377972 0.93490294 0.91441173 0.90310169\n",
      "  0.92449709 0.95086103 0.90310573 0.93733777 0.9078094  0.92630362\n",
      "  0.93223847 0.906189   0.91153026 0.90253342 0.90785227 0.92965935]\n",
      " [0.94604452 0.93886115 0.91810603 0.94199288 0.93376237 0.96795272\n",
      "  0.88956497 0.95994024 0.94415754 0.95252993 0.93349964 0.92587465\n",
      "  0.94668096 0.92483912 0.94791325 0.94896884 0.90635298 0.93838055\n",
      "  0.94349039 0.97101166 0.95399219 0.92317241 0.93082706 0.92485527\n",
      "  0.93342979 0.94752067 0.94993263 0.9129646  0.9128978  0.94921961]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(30,10)\n",
    "B = np.random.rand(6,10)\n",
    "C = B @ A.T\n",
    "print(C.shape)\n",
    "y_h = 1 / (1 + np.exp(-C))\n",
    "print(y_h.shape)\n",
    "print(y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00626879 0.01704033 0.04632042 0.93037047]\n",
      " [0.01203764 0.08894682 0.24178252 0.65723302]\n",
      " [0.00626879 0.01704033 0.04632042 0.93037047]\n",
      " [0.         0.         1.         0.        ]]\n",
      "Summe:  1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    #print(\"s max: \", s)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting. Transform array into next dimension as column vector\n",
    "    #print(\"s new: \", s)\n",
    "    e_x = np.exp(z - s)\n",
    "    #print(\"e_x\", e_x)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    #print(\"div sum\", div)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    #print(\"div\", div)\n",
    "    return e_x / div\n",
    "\n",
    "x2 = np.array([[1, 2, 3, 6],  # sample 1\n",
    "               [2, 4, 5, 6],  # sample 2\n",
    "               [1, 2, 3, 6],\n",
    "            [1, 2, 1000, 6]]) # sample 1 again(!)\n",
    "\n",
    "sm = softmax(x2)\n",
    "print(sm)\n",
    "print(\"Summe: \",sum(sm[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98532365, 0.35034106, 0.60193241, 0.65464954],\n",
       "       [0.08561235, 0.50530124, 1.2144773 , 0.53986865],\n",
       "       [1.01639745, 0.18541299, 0.25126594, 0.58106588]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = abs(np.random.randn(3, 4))\n",
    "weights.shape\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w, X):\n",
    "    Z = w @ X.T\n",
    "    y_head = 1 / (1 + np.exp(-Z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3,    6],\n",
       "       [   2,   99,    5,    6],\n",
       "       [   1,    2,    3,    6],\n",
       "       [   1,    2, 1000,    6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2[1][1]= 99\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.sort(np.array([4,2,1,4]))\n",
    "we = np.zeros((len(y), 3))\n",
    "we\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, label in enumerate(y):\n",
    "    we[i][label] = 1\n",
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_head = sigmoid(weights, X)\n",
    "print(y_head.shape)\n",
    "prob = softmax(y_head.T)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4]\n",
      " [3 3 3]]\n",
      "[[4 4 4]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "T = np.array([[1,2,3], [2,2,2]])\n",
    "U = np.array([[4,4,4], [1,2,3]])\n",
    "print(T)\n",
    "print(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[ 4  8 12]\n",
      " [ 2  4  6]]\n"
     ]
    }
   ],
   "source": [
    "I = np.multiply(T,U)\n",
    "print(I.shape)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.sum(I, axis=1)\n",
    "print(s)\n",
    "s1 = np.sum(s)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27558178, 0.27569065, 0.27678485])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.multiply(we, np.log10(prob)) + np.multiply((1 - we), (np.log10(1 - prob)))\n",
    "loss_v = -(np.sum(L, axis=0) / len(X))\n",
    "loss_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = we\n",
    "probability_distribution = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9066723370880125"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.multiply(groundtruth, np.log(probability_distribution)) + np.multiply((1 - groundtruth), (np.log(1 - probability_distribution)))\n",
    "loss_v = np.sum(L, axis=0)\n",
    "loss_v1 = np.sum(loss_v)\n",
    "loss_v1_final = -(loss_v1) / len(groundtruth)\n",
    "loss_v1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0966967763919089"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_new = np.multiply(groundtruth, np.log(probability_distribution))\n",
    "L_new = -(np.sum(L_new)) / len(groundtruth)\n",
    "L_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0966768181918065"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_skilearn = log_loss(y, y_head.T)\n",
    "l_skilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27558178, 0.2779912 , 0.27574518],\n",
       "       [0.2768402 , 0.27569065, 0.27677651],\n",
       "       [0.27688625, 0.27563625, 0.27678485]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost0 = we.T.dot(np.log10(prob))\n",
    "cost1 = (1-we).T.dot(np.log10(1-prob))\n",
    "cost = -((cost1 + cost0))/len(we) \n",
    "cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(we.shape)\n",
    "print(prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99927236, 0.98224928, 0.99816808],\n",
       "       [0.99894456, 0.97687956, 0.99753863],\n",
       "       [0.99872757, 0.97601703, 0.99702119],\n",
       "       [0.99871071, 0.97996312, 0.99680612],\n",
       "       [0.9992247 , 0.98296642, 0.99800966],\n",
       "       [0.99965521, 0.99110684, 0.99896388],\n",
       "       [0.99884529, 0.98155002, 0.99707615],\n",
       "       [0.99921701, 0.98330444, 0.99798577],\n",
       "       [0.99821232, 0.97467495, 0.99583901],\n",
       "       [0.99897548, 0.97939976, 0.99750292],\n",
       "       [0.99952458, 0.98609646, 0.99873028],\n",
       "       [0.99910226, 0.98493351, 0.99759393],\n",
       "       [0.99875671, 0.97542391, 0.99711352],\n",
       "       [0.99756533, 0.96352421, 0.99483796],\n",
       "       [0.99965426, 0.9834243 , 0.99913722],\n",
       "       [0.9997571 , 0.99141383, 0.99926771],\n",
       "       [0.99956138, 0.98562449, 0.99885446],\n",
       "       [0.99931844, 0.98316648, 0.99827132],\n",
       "       [0.9997163 , 0.99038441, 0.99917517],\n",
       "       [0.99942222, 0.98713688, 0.99840521],\n",
       "       [0.99953179, 0.98729428, 0.99872345],\n",
       "       [0.99943953, 0.9871807 , 0.99846698],\n",
       "       [0.99853813, 0.97167708, 0.99669956],\n",
       "       [0.99946456, 0.98832617, 0.99851844],\n",
       "       [0.99925047, 0.98948565, 0.99776824],\n",
       "       [0.9991519 , 0.98192709, 0.99788475],\n",
       "       [0.99935314, 0.98668155, 0.9982508 ],\n",
       "       [0.99937909, 0.98437951, 0.99838584],\n",
       "       [0.9993171 , 0.98150253, 0.99831391],\n",
       "       [0.99893757, 0.98321727, 0.99723688],\n",
       "       [0.99900287, 0.9825105 , 0.99745665],\n",
       "       [0.9995367 , 0.98548562, 0.99880484],\n",
       "       [0.99946272, 0.98778285, 0.99846925],\n",
       "       [0.99961595, 0.98788803, 0.99892773],\n",
       "       [0.99904034, 0.98046126, 0.99764355],\n",
       "       [0.99899419, 0.97367022, 0.99774656],\n",
       "       [0.99947883, 0.98066315, 0.99874829],\n",
       "       [0.99908666, 0.98188689, 0.99766572],\n",
       "       [0.99816686, 0.97286424, 0.99581163],\n",
       "       [0.99929043, 0.9834444 , 0.99818008],\n",
       "       [0.99920129, 0.98087403, 0.99803814],\n",
       "       [0.99801202, 0.9640228 , 0.99593474],\n",
       "       [0.99829069, 0.97540843, 0.99596349],\n",
       "       [0.999452  , 0.98861154, 0.99847098],\n",
       "       [0.99957457, 0.99246193, 0.9986388 ],\n",
       "       [0.99890912, 0.97788361, 0.99742939],\n",
       "       [0.99941917, 0.98796591, 0.99835182],\n",
       "       [0.99867791, 0.97852218, 0.99678509],\n",
       "       [0.99947538, 0.9859786 , 0.99859463],\n",
       "       [0.99913885, 0.98023341, 0.99789606],\n",
       "       [0.99999222, 0.9998301 , 0.99993888],\n",
       "       [0.99998516, 0.99978396, 0.99988841],\n",
       "       [0.99999262, 0.99986605, 0.99993815],\n",
       "       [0.99992393, 0.99924858, 0.99958094],\n",
       "       [0.99998543, 0.99976781, 0.99989413],\n",
       "       [0.9999612 , 0.99968726, 0.99972507],\n",
       "       [0.99998686, 0.99984604, 0.99989119],\n",
       "       [0.999754  , 0.99793358, 0.99892631],\n",
       "       [0.99998547, 0.99975618, 0.99989455],\n",
       "       [0.99991161, 0.99932611, 0.99948935],\n",
       "       [0.99977263, 0.99803286, 0.99900653],\n",
       "       [0.99996878, 0.99964093, 0.99979244],\n",
       "       [0.99994142, 0.99910975, 0.99969425],\n",
       "       [0.99997902, 0.99978646, 0.99983871],\n",
       "       [0.99992893, 0.99910586, 0.99962547],\n",
       "       [0.99998703, 0.99973607, 0.99990892],\n",
       "       [0.99996498, 0.99974405, 0.9997389 ],\n",
       "       [0.99994362, 0.99937684, 0.99966698],\n",
       "       [0.99997434, 0.99963579, 0.99983541],\n",
       "       [0.9999222 , 0.99915303, 0.99957985],\n",
       "       [0.99998333, 0.99986677, 0.9998555 ],\n",
       "       [0.99996465, 0.99944546, 0.99979239],\n",
       "       [0.99998354, 0.99980906, 0.9998728 ],\n",
       "       [0.99997524, 0.99974979, 0.99981545],\n",
       "       [0.9999788 , 0.99964298, 0.99986066],\n",
       "       [0.99998517, 0.99972001, 0.99989729],\n",
       "       [0.99998974, 0.99981264, 0.99992133],\n",
       "       [0.99999231, 0.99988605, 0.99993297],\n",
       "       [0.99997554, 0.99973985, 0.99982286],\n",
       "       [0.99990754, 0.99863101, 0.99956339],\n",
       "       [0.99990557, 0.99898562, 0.99951417],\n",
       "       [0.99989293, 0.99879133, 0.99947202],\n",
       "       [0.99994421, 0.9992869 , 0.99968823],\n",
       "       [0.99998288, 0.9998684 , 0.99985081],\n",
       "       [0.99995735, 0.99973963, 0.99968007],\n",
       "       [0.99998077, 0.99980854, 0.99984765],\n",
       "       [0.99998986, 0.99982628, 0.9999203 ],\n",
       "       [0.99997282, 0.99956818, 0.99983189],\n",
       "       [0.99994921, 0.99953663, 0.99967574],\n",
       "       [0.99992908, 0.99932076, 0.99959619],\n",
       "       [0.99994253, 0.99958057, 0.99962006],\n",
       "       [0.99997849, 0.99977077, 0.99983765],\n",
       "       [0.9999456 , 0.99933568, 0.99969028],\n",
       "       [0.99976913, 0.9978452 , 0.99901184],\n",
       "       [0.99994688, 0.99952246, 0.9996657 ],\n",
       "       [0.99995373, 0.99957054, 0.99969725],\n",
       "       [0.99995512, 0.99957202, 0.99970899],\n",
       "       [0.99997418, 0.99963682, 0.99982926],\n",
       "       [0.99978115, 0.99736744, 0.99912475],\n",
       "       [0.99995064, 0.99949175, 0.99969601],\n",
       "       [0.99999667, 0.99998047, 0.99995347],\n",
       "       [0.99998287, 0.99988614, 0.99984643],\n",
       "       [0.99999768, 0.99997026, 0.99997178],\n",
       "       [0.99999229, 0.99994329, 0.99991679],\n",
       "       [0.99999583, 0.9999665 , 0.99994975],\n",
       "       [0.99999907, 0.99998782, 0.99998576],\n",
       "       [0.99992704, 0.99968596, 0.99948064],\n",
       "       [0.99999811, 0.99997775, 0.99997474],\n",
       "       [0.9999947 , 0.99994739, 0.99994324],\n",
       "       [0.99999884, 0.99998624, 0.99998281],\n",
       "       [0.99999324, 0.99992108, 0.99993516],\n",
       "       [0.99999159, 0.99991516, 0.99992063],\n",
       "       [0.99999603, 0.9999504 , 0.99995767],\n",
       "       [0.99997983, 0.99986409, 0.9998293 ],\n",
       "       [0.99998807, 0.99991736, 0.99988726],\n",
       "       [0.99999457, 0.9999469 , 0.99994266],\n",
       "       [0.99999351, 0.99994016, 0.99993164],\n",
       "       [0.99999944, 0.99999324, 0.9999898 ],\n",
       "       [0.99999929, 0.99999078, 0.99998856],\n",
       "       [0.99997687, 0.9997981 , 0.99982212],\n",
       "       [0.99999739, 0.9999687 , 0.9999688 ],\n",
       "       [0.99997872, 0.99986699, 0.99981672],\n",
       "       [0.99999909, 0.99998751, 0.9999862 ],\n",
       "       [0.99998739, 0.99985321, 0.99989704],\n",
       "       [0.9999965 , 0.99996628, 0.99995785],\n",
       "       [0.99999775, 0.99997224, 0.99997148],\n",
       "       [0.99998573, 0.99984107, 0.99988527],\n",
       "       [0.99998618, 0.99987168, 0.99988066],\n",
       "       [0.99999405, 0.99994971, 0.99993567],\n",
       "       [0.9999969 , 0.99995639, 0.99996504],\n",
       "       [0.99999813, 0.99997198, 0.99997694],\n",
       "       [0.99999937, 0.99998934, 0.99998991],\n",
       "       [0.99999443, 0.99995235, 0.9999393 ],\n",
       "       [0.99998687, 0.99987129, 0.99988558],\n",
       "       [0.99998644, 0.99991669, 0.99986399],\n",
       "       [0.999999  , 0.99998011, 0.99998701],\n",
       "       [0.99999563, 0.99996814, 0.99994648],\n",
       "       [0.99999308, 0.99994262, 0.99992572],\n",
       "       [0.9999838 , 0.99985386, 0.99986453],\n",
       "       [0.99999631, 0.99994721, 0.99996151],\n",
       "       [0.99999673, 0.99996417, 0.99996232],\n",
       "       [0.99999612, 0.99993178, 0.99996305],\n",
       "       [0.99998287, 0.99988614, 0.99984643],\n",
       "       [0.99999745, 0.99997524, 0.99996716],\n",
       "       [0.99999731, 0.99997283, 0.99996659],\n",
       "       [0.9999954 , 0.99993535, 0.99995501],\n",
       "       [0.99998808, 0.99986373, 0.99990168],\n",
       "       [0.99999318, 0.99992267, 0.99993438],\n",
       "       [0.99999419, 0.99995676, 0.99993397],\n",
       "       [0.99998508, 0.99989761, 0.99986093]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_head.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 150]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# verify result\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss_skilearn \u001b[38;5;241m=\u001b[39m \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m loss_skilearn\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2592\u001b[0m, in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   2587\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   2588\u001b[0m     y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat16]\n\u001b[0;32m   2589\u001b[0m )\n\u001b[0;32m   2590\u001b[0m eps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(y_pred\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps \u001b[38;5;28;01mif\u001b[39;00m eps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m eps\n\u001b[1;32m-> 2592\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2593\u001b[0m lb \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n\u001b[0;32m   2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 150]"
     ]
    }
   ],
   "source": [
    "# verify result\n",
    "loss_skilearn = log_loss(y, y_head.T)\n",
    "loss_skilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(we.shape)\n",
    "print(prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]] \n",
      "Shape:  (5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  5,  4],\n",
       "       [ 7,  6,  5],\n",
       "       [ 8,  7,  6],\n",
       "       [ 9,  8,  7],\n",
       "       [10,  9,  8]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([1, 2, 3, 4, 5])\n",
    "x2 = np.array([5, 4, 3])\n",
    "\n",
    "print(x1.shape)\n",
    "x1_new = x1[:,np.newaxis]\n",
    "print(x1_new, \"\\nShape: \", x1_new.shape)\n",
    "x3 = x1_new + x2\n",
    "x3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
